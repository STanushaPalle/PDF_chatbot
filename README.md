# ğŸ§  Streamlit PDF RAG Application

A **privacy-first**, **local LLM-powered** application to chat with, summarize, and compare PDFs using:

- ğŸ” **LangChain** for Retrieval-Augmented Generation (RAG)
- ğŸ’¬ **Ollama** for local large language model (LLM) inference
- ğŸ“„ **Streamlit** for an interactive, browser-based UI
- ğŸ“š **ChromaDB** for persistent vector storage

---

## ğŸ“¦ Features

- Upload or select sample PDFs
- Extract and chunk text from documents
- Generate local vector embeddings using `nomic-embed-text`
- Store/retrieve content via ChromaDB vector store
- Chat with PDFs using a LangChain + ChatOllama RAG pipeline
- Summarize document content
- Compare two PDFs across tone, purpose, and structure
- 100% local â€” no external API or cloud required

---

## ğŸ–¼ï¸ Architecture


```text
User Interface (Streamlit)
        |
        v
PDF Upload/Selection (sample or user-provided)
        |
        v
PDF Processing (pdfplumber + UnstructuredPDFLoader)
        |
        v
Text Chunking (LangChain RecursiveCharacterTextSplitter)
        |
        v
Embeddings Generation (Ollama + nomic-embed-text)
        |
        v
Vector Store (ChromaDB, with persistent directory)
        |
        v
Retrieval and MultiQuery Optimization (LangChain Retriever)
        |
        v
Chat + Summary + Comparison Chains (ChatOllama + Prompt Templates)
        |
        v
Chat UI + Summary UI + Compare UI (Streamlit containers and widgets)
```

---
## Project Structure
```
ollama_pdf_rag/
â”œâ”€â”€ src/                      # Source code
â”‚   â”œâ”€â”€ app/                  # Streamlit application
â”‚   â”‚   â”œâ”€â”€ components/       # UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py      # Chat interface
â”‚   â”‚   â”‚   â”œâ”€â”€ pdf_viewer.py # PDF display
â”‚   â”‚   â”‚   â””â”€â”€ sidebar.py   # Sidebar controls
â”‚   â”‚   â””â”€â”€ main.py          # Main app
â”‚   â””â”€â”€ core/                 # Core functionality
â”‚       â”œâ”€â”€ document.py       # Document processing
â”‚       â”œâ”€â”€ embeddings.py     # Vector embeddings
â”‚       â”œâ”€â”€ llm.py           # LLM setup
â”‚       â””â”€â”€ rag.py           # RAG pipeline
â”œâ”€â”€ data/                     # Data storage
â”‚   â”œâ”€â”€ pdfs/                # PDF storage
â”‚   â”‚   â””â”€â”€ sample/          # Sample PDFs
â”‚   â””â”€â”€ vectors/             # Vector DB storage
â”œâ”€â”€ notebooks/               # Jupyter notebooks
â”‚   â””â”€â”€ experiments/         # Experimental notebooks
â”œâ”€â”€ tests/                   # Unit tests
â”œâ”€â”€ docs/                    # Documentation
â””â”€â”€ run.py                   # Application runner
```

## ğŸš€ Getting Started

### Prerequisites

1. **Install Ollama**
   - Visit [Ollama's website](https://ollama.ai) to download and install
   - Pull required models:
     ```bash
     ollama pull llama3.2  # or your preferred model
     ollama pull nomic-embed-text
     ```

2. **Clone Repository**
   ```bash
   git clone https://github.com/STanushaPalle/PDF_chatbot
   cd PDF_chatbot
   ```

3. **Set Up Environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: .\venv\Scripts\activate
   pip install -r requirements.txt
   ```

   Key dependencies and their versions:
   ```txt
   ollama==0.4.4
   streamlit==1.40.0
   pdfplumber==0.11.4
   langchain==0.1.20
   langchain-core==0.1.53
   langchain-ollama==0.0.2
   chromadb==0.4.22
   ```

### ğŸ® Running the Application

#### Option 1: Streamlit Interface
```bash
python run.py
```
Then open your browser to `http://localhost:8501`

![Streamlit UI](st_app_ui.png)
*Streamlit interface showing PDF viewer and chat functionality*

#### Option 2: Jupyter Notebook
```bash
jupyter notebook
```
Open `updated_rag_notebook.ipynb` to experiment with the code

## âš™ï¸ Configuration

By default, the app connects to:

- `http://localhost:11434` for **Ollama**
- A persistent `chroma/` directory for **vectorstore**

You can configure these in a `.env` file (optional):

```env
OLLAMA_HOST=http://localhost:11434
VECTOR_DIR=chroma
```

# ğŸ“ Project Overview

## â— Known Issues

| Challenge              | Solution                                          |
|------------------------|---------------------------------------------------|
| Ollama not running     | Ensure Ollama is active at `localhost:11434`     |
| Slow inference         | Use smaller/faster LLMs (e.g., `phi3`)           |
| UI bugs or freeze      | Clear session state or restart the app           |
| PDF with no text       | Fallback to OCR or `UnstructuredPDFLoader`       |

## ğŸ”® Future Roadmap

- âœ… Memory-enabled chat  
- ğŸ§  Metadata-based retrieval filters  
- ğŸ“Š Table/image-aware chunking  
- ğŸ”„ Cross-PDF querying  
- ğŸ›¡ï¸ User roles + secure document upload  

## ğŸ“œ License

MIT License â€“ Open source, free to use and modify.

## ğŸ¤ Acknowledgements

- [Ollama](https://ollama.com)
- [LangChain](https://www.langchain.com)
- [Streamlit](https://streamlit.io)
- [ChromaDB](https://www.trychroma.com)
- [nomic-embed-text](https://github.com/nomic-ai/nomic)

